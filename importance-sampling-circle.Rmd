---
title: "Importance Sampling for Stochastic Simulation on a Noisy Circle"
output:
  html_document: default
  html_notebook: default
---
## Remark

There are several clearly marked "todo"s that we can improve upon if the need arises.

## Outline

We first discuss the setup of the algorithm and the two estimators: the naive estimator and the importance sampling estimator.

We define the utility functions and the naive estimator. 

We then present the importance sampling algorithm step-by-step, with visualizatin to aid understanding of the algorithm.

Finally we present the algorithm as a whole, without visualization. We run the algorithm multiple times (in the thousands) to obtain empirical standard errors of the estimators. We plot standard errors with respect to the number of simulations. We will see that the importance sampling estimator results in a lower variance than the naive estimator, conditioning on the number of simulations.

## Setup

In this note, we provide a more elaborate example than [the previous note](https://ruqianchen.github.io/importance-sampling-mse.html). 

We sample $X$ from its "natural distribution". We then sample $Y$ from a family of distributions parametrized by $x$, hence the name "stochastic simulation". Finally, we evaluate the function $s(y)$. We are interested in $E[s(Y)]$. 

The natural distribution of $X$ is a noisy circle. More specifically, $X=u+\epsilon$ where \begin{align}
u&=(\cos(\theta),\sin(\theta)),\\
\theta&\sim \text{Unif}(0,2\pi),\\
\epsilon&\sim N\left((0,0), \begin{bmatrix}0.01 & 0\\0 &0.01\end{bmatrix}\right),\\
u&\perp \epsilon.
\end{align}

We define $Y|X=x \sim N(\exp(x_1+x_2), 1)$. 

Let $\zeta=1.35$. We define $s(y) = 1(y>\zeta)$. The $\zeta$ is chosen such that $P(s=1)=0.5$. (Todo: Note that I chose the value $1.35$ by a rough numerical approximation. I will try to calculate algebraically what $\zeta$ should be such that $P(s==1)=0.5$.)

The naive estimator is obtained as follows. We pick a positive integer $n$. We sample $n$ many iid $x$'s from the natural distribution. For each $x$, we sample a $y$ from the distribution $Y|X=x$ and compute $s(y)$. The mean of the $n$ many $s(y)$'s gives the naive estimator.

The importance sampling estimator is obtained as follows. We pick positive integers $n,m,k$ such that $m\approx n^{2/3}$ and $k\approx \sqrt{m}$. In the first stage, we sample $m$ many iid $s$'s. For each $x$, we sample a $y$ from the distribution $Y|X=x$ and compute $s(y)$. We perform $k$-means clustering and construct a Gaussian mixture on the $m$ many sampled $x$'s, using the same method as in the [the previous note](https://ruqianchen.github.io/importance-sampling-mse.html). In the second stage, we sample $(n-m)$ many iid $x$'s from the Gaussian mixture. For each $x$, we sample a $y$ from $Y|X=x$ and compute $s(y)$. We compute the importance sampling estimator using the weighted mean of the second stage $s(y)$'s. The weights are chosen such that the estimator is unbiased. 

## Packages and auxilary functions.

```{r load myData, echo=FALSE}
load("is-circle-3.RData")
```

### Load packages 

```{r load package, message=FALSE, warning=FALSE} 
library(distr)
library(plyr)
library(mclust)
library(MASS)
library(ggplot2)
library(mvtnorm)
library(fields)
```

### Define utility functions

For ease of computation, we will approximate the probability density function of the natural distribution of $x$ with a Gaussian mixture. Note that this is different from the Gaussian mixture we will obtain for the second stage sampling of the $x$'s.

```{r circle density}
## FUNCTION: circle.density
## ARGS: x.tmp (the sample point), n.cut (the number of equally spaced points on the unit circle)
## RETURNS: the Gaussian mixture pdf value of x.tmp, when the mixture has n.cut many normal distributions
##  each centered at one of the equally spaced points on the unit circle, with variance 0.01 * I_2

circle.density <- function(x.tmp = c(0,0), n.cut = 100){
  theta.tmp <- seq(0,2*pi, by = 2*pi/n.cut)   
  u.tmp <- cbind(cos(theta.tmp), sin(theta.tmp))
  d.tmp <- sum(unlist(lapply(1:nrow(u.tmp), function(x){dmvnorm(x.tmp, mean = u.tmp[x,], sigma = 0.01 * diag(2))})))/n.cut
  return(d.tmp)
}

### TEST CODE ######################
# # > circle.density(c(0,1),1000)
# # [1] 0.6357345
# # > circle.density(c(0,1),100000)
# # [1] 0.6357345
####################################

## FUNCTION: circle.density.df
## ARGS: x.tmp (an two column dataframe of sample poitns), n.cut (the number of equally spaced points on the unit circle)
## RETURNS: A vector of the Gaussian mixture pdf values of the poitns in x.tmp, 
##   when the mixture has n.cut many normal distributions
##  each centered at one of the equally spaced points on the unit circle, with variance 0.01 * I_2
## REMARK: This function is an extension of `circle.density`. 
##  This function can compute handle a dataframe of multiple sample points

circle.density.df <- function(x.tmp = c(0,0), n.cut = 100){
  theta.tmp <- seq(0, 2*pi, length.out = n.cut + 1)   
  u.tmp <- cbind(cos(theta.tmp), sin(theta.tmp))
  d.tmp <- dmvnorm(x.tmp, u.tmp[1,], 0.01 * diag(2))
  for (j in 2:n.cut){
    d.tmp <- d.tmp + dmvnorm(x.tmp, u.tmp[j,], 0.01 * diag(2))
  }
  d.tmp <- d.tmp / n.cut
  return(d.tmp)
}

### TEST CODE ######################################################
# > circle.density.df(data.frame(a=c(0,1,2,3), b=c(1,3,5,9)))
# 1             2             3             4 
# 6.357345e-01 1.063925e-102  0.000000e+00  0.000000e+00 
# > circle.density.df(data.frame(a=c(0,1,2,3), b=c(1,3,5,9)), 10000)
# 1             2             3             4 
# 6.357345e-01 1.063925e-102  0.000000e+00  0.000000e+00 
#####################################################################
```

Define $\zeta=1.35$. Set up a function to sample from the family of distributions $Y|X=x$ for varying $x$'s.

```{r define y and zeta}
zeta <- 1.35 # this is chosen to be the median of one trial y. Goal is this represents P(Y>zeta)= 0.5.
v <- function(x) {
    return(rnorm(1, exp(x[1] + x[2]), 1)) 
}
```

## The naive estimator

```{r naive estimator}
## FUNCTION: sample.once.naive
## ARGS: number of total samples
## RETURNS: the naive estimator

sample.once.naive <- function(n=2000){
  m <- n
  theta <- runif(m, 0, 2 * pi) # sample angle theta uniformly from 0 to 2pi
  u <- cbind(cos(theta), sin(theta)) # turning angles into uniformly sampled points on the unit circle
  epsilon <- mvrnorm(m, mu = c(0, 0), Sigma = 0.01 * diag(2)) # define a noise in 2D
  x <- u + epsilon # x is the sum of the points in the unit circle and some noise
  y <- unlist(lapply(1:m, function(j) v(x[j,]))) # these are the sampled y's. 
  s <- ifelse(y>zeta, 1, 0) # evaluate s using the y's
  return(mean(s))
}
```


## The importance sampling estimator with visualization

### Define sampling distributions.

We will perform two stages of sampling. We set the total number $n$ of samples to be $2000$. We set the first stage number $m$ of samples to be $\sqrt{n^{2/3}}\approx 158$. 

```{r}
n <- 2000 # total samples
m <- floor (n^(2/3)) # first-stage samples, m = 158
```

### First-stage Sampling
We sample $m=158$ many $X$ from the natural distribution in the following steps.

We first sample points uniformly on a circle.

```{r}
theta <- runif(m, 0, 2 * pi)
u <- cbind(cos(theta), sin(theta))
## ----plot u--------------------------------------------------------------------
plot(u, main = "Sampled Points on the Unit Circle", col = "purple", pch = 20)
grid()
```

We then add noise to the points on the unit circle.

```{r}
epsilon <- mvrnorm(m, mu = c(0, 0), Sigma = 0.01 * diag(2))
x <- u + epsilon
## ----plot x--------------------------------------------------------------------
plot(x, main = "Sampled Noisy x's", col = "purple", pch = 20)
grid()
```

For each sampled $x$, we sample a $v$ from the distribution defined above. We compute the mean and median of $y$'s. We note that the median is close to $\zeta$.

```{r}
x.old <- x
y <- unlist(lapply(1:m, function(x) v(x.old[x, ]))) # these are the sampled V's.
mean(y) # around 1.525384
median(y) # around 1.377392
```

We define a vector $s$ where $s_j = 1$ if $y_j>\zeta$ and $s_j=0$ otherwise.

We visualize the $Y$'s in the following plot. We plot the $m$ many sampled $x$'s, where $x$ is colored blue when $y>\zeta=1.35$ and colored red when $y\leq \zeta$.

Recall that $Y|X=x$ is a one-dimensional normal distribution with mean $\exp(x_1+x_2)$ and variance $1$. Therefore we expect to see mostly blue points in regions where $|x_1+x_2|$ is large, e.g. the first quadrant. We expect to see mostly red points inn regions where  $|x_1+x_2|$ is small, e.g. the third quadrant. 

```{r}
s <- ifelse(y > zeta, 1, 0)

## ----plot y---------------------------------------------------------------
color <- ifelse(s == 1, "blue", "red")
plot(x.old,
     col = color,
     pch = 20,
     main = "First Stage Samples of x, Color by Values of y's.")
grid()
legend(
  'bottomright',
  c("y>zeta", "y<=zeta"),
  pch = 20,
  col = c("blue", "red"),
  bty = 'y',
  cex = .65
)
```

### K-means clustering

We perform k-means clustering on the $x$'s where $k$ is chosen to be $\sqrt{m}$. We examine the size of of each cluster. Because $x$'s are sampled from a noisy unit circle that is roughly evenly spread apart, we expect the cluster sizes to be similar.

We observe that the sizes of clusters are indeed more or less spread evenly with no cluster with extremely large or small size.

```{r}
size <- floor(sqrt(m))  # number of k-means clusters
model <- kmeans(x.old, size)
cluster.size.count <-
  unlist(lapply(1:size, function(x) {
    length(which(model$cluster == x))
  }))
      
## ----plot the size of clusters--------------------------------------------------------------------
barplot(
  cluster.size.count,
  main = "Size of Clusters",
  xlab = "Cluster index",
  names.arg = 1:size,
  col = "purple"
)
```

We plot the points $x$'s, colored according to which clusters they belong to. The numbers indicate cluster indices. The positions of the numbers are the cluster centers.

```{r}     
## ----plot2---------------------------------------------------------------
plot(x.old,
     col = model$cluster,
     # xlim = c(-1,1),
     # ylim = c(-1,1),
     pch = 20)
# points(model$centers, col = "black", pch = 2)
grid()
text(
  model$centers[, 1],
  model$centers[, 2],
  labels = as.character(1:size),
  col = "black",
  pos = c(1),
  offset = -0.16
)
```

For each cluster, we compute the proportion of $x$'s whose  corresponding $y$'s are greater than $\zeta$. We call this the "cluster probability". We expect higher probabilities from the clusters in regions where $|x_1+x_2|$ is large, e.g. the first quadrant. We expect lower probabilities from the clusters in regions where $|x_1+x_2|$ is small, e.g. the third quadrant. 

```{r}
cluster.prob <-
  unlist(lapply(1:nrow(model$centers), function(x) {
    mean(s[which(model$cluster == x)])
  }))
      
## ----plot---------------------------------------------------------------
barplot(
  cluster.prob,
  main = "Proportion of Y>zeta's in the Clusters",
  xlab = "Cluster index",
  names.arg = 1:size,
  col = "purple"
)
```

We visualize the probabilities of the clusters in the following scatterplot. As expected, the clusters in the first quadrant have probabilities close to $100%$, while those in the third quadrant have probabilities close to $0%$. 

We use a grayscale to color the probabilities, with black indicating $100%$ and white indicating $0%$.

```{r}
## ----plot probabilities of the clusters---------------------------------------------------------------
pos.prob <- which(cluster.prob > 0)
colors.grayscale <-
  paste("gray", floor((1 - cluster.prob) * 100), sep = "")
plot(
  x.old[which(model$cluster == pos.prob[1]), ],
  col = colors.grayscale[1],
  bg = colors.grayscale[1],
  xlim = c(-1.5, 1.5),
  ylim = c(-1.5, 1.5),
  main = "Clusters and Their Respective Probabilities",
  xlab = "x_1",
  ylab = "x_2",
  pch = 20
)
grid()
for (i in 2:size) {
  points(x.old[which(model$cluster == i), ],
         col = colors.grayscale[i],
         bg = colors.grayscale[i],
         pch = 20)
}
for (i in 1:size) {
  points(x.old[which(model$cluster == i), ], col = "purple")
}
text(
  model$centers[, 1],
  model$centers[, 2],
  labels = paste(
    paste("#", as.character(1:size), ":", sep = ""),
    paste(floor(cluster.prob * 100), rep("%", size), sep = ""),
    sep = " "
  ),
  col = "red",
  pos = c(1),
  offset = -0.16
)
# text(model$centers[,1], model$centers[,2], labels=as.character(1:size), col="red", pos=c(1), offset=-0.16)
# text(model$centers[,1],
#      model$centers[,2],
#      labels=paste(floor(cluster.prob*100), rep("%", size), sep = ""),
#      col="red",
#      pos=c(4),
#      offset=1)
```

### Store Information

We prepare a data frame to store the information we have obtained so far. Note that here instead of using old cluster centers, we can update any cluster center to be the mean of the points belonging to  that cluster. However, for convenience, we stick with the old cluster centers for this example.

The `prob` column corresponds to the bar plot above.

```{r}
df <- data.frame(model$centers)
names(df) <- c("centers.old1", "centers.old2")
df$prob <- cluster.prob
# centers.new <- matrix(unlist(lapply(1:size,function(x){colMeans(x.old[which(model$cluster == x),])})),,2)
# df$centers.new1 <- centers.new[,1]
# df$centers.new2 <- centers.new[,2]
df$centers.new2 <- df$centers.old2
df$centers.new1 <- df$centers.old1
```

### Second-stage Sampling

We prepare a Gaussian Mixture Model (GMM). Our second stage sampling will be generated from this GMM. The pdf of a GMM is a linear combination of several normal distributions. 

We will define $m$ many bivariate normal distributions. We will also define $m$ weights that sum to $1$.

First, we define the $\sqrt{m}$ many normal distributions by specifying the means and the covariance matrices. We save them in our data frame.

```{r}
sigma.new <- function(i) {
  cov(x.old[which(model$cluster == i), ])
}
sigma.array <- lapply(1:size, function(x) unlist(sigma.new(x)))
mean.array <- lapply(1:size, function(x) cbind(df$centers.new1, df$centers.new2)[x, ])
df$sigma <- sigma.array
df
```

Second, we define the weight vector.

The weight vector is $\pi=\langle \pi_1,\cdots,\pi_{\text{size}}\rangle$ for the Gaussian Mixture Model. The $\pi_j$'s sum to $1$.

```{r}
cluster.size.percentage <-
  unlist(lapply(1:size, function(x) {
    sum(s[which(model$cluster == x)]) / sum(s)
  }))
sum(cluster.size.percentage) # = 1
```

We visualize the weight vector.
```{r}
## ----plot5--------------------------------------------------------------------
barplot(
  cluster.size.percentage,
  main = "Probability of Clusters",
  xlab = "Cluster index",
  names.arg = 1:size,
  col = "purple"
)
```

Third, we generate $n-m$ many new $x$'s.

We sample uniformly $n-m$ many numbers from $1,2,\cdots,\sqrt{m}$ with replacement. We call this the compenent vector.

We verify the distribution of the components. The bars should only be positive for those clusters with positive probability $\pi$. More specifically it should look similar to the bar plot above. We fit a density plot on top of the histogram.

```{r}
components <- 
  sample(1:size,
         prob = cluster.size.percentage,
         size = (n - m),
         replace = TRUE)
      
## ----plot histogram of components--------------------------------------------------------------------
hist(
  components,
  col  = "purple",
  breaks = c(0:(size + 1)),
  prob = TRUE,
  xaxt = 'n',
  xlab = "Component Index"
)
lines(density(components))
axis(1,
     at = seq(0.5, (size + 1.5), by = 1),
     labels = c(1:(size + 2)),
     las = 1)
```

Finally we are ready to construct the GMM. 

We can visualize the GMM in the density plot.

```{r}
## ------------------------------------------------------------------------
# https://stackoverflow.com/questions/38761453/confusion-on-2-dimension-kernel-density-estimation-in-r
# https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/kde2d.html
# https://stackoverflow.com/questions/30814545/r-get-joint-probabilities-from-2d-kernel-density-estimate
density <- kde2d(x.new[, 1], x.new[, 2], n = 50)
points.tmp <- data.frame(x = x.new[, 1], y = x.new[, 2])
p.old2 <- interp.surface(density, points.tmp)
      
## ----plot gmm kde ----------------------------------------------------------------
filled.contour(density, color.palette = colorRampPalette(c(
  'white', 'blue',
  'yellow', 'red',
  'darkred'
)))
```

Alternatively we can visualize the GMM by plotting each normal distribution in the GMM. Each normal distribution should line up well with the underlying original cluster points that the normal distribution was constructed upon.

Each cluster comes with two plots. 

The first one shows the original sampled $x$'s, colored by cluster indices. It also shows the solid red-blue gradient colored points from the normal distribution. Blue indicates high pdf values while red indicates low pdf values.

The second plot shows the contour plot of the normal distribution pdf.

```{r}
## ----plot to visualize.gmm--------------------------------------------------------------------
plot.cluster <- function(counter){ # counter indicate cluster indices

num.sample <- 300 # sample 300 times from each normal.

this.cluster.points <- rmvnorm(
  num.sample, mean = unlist(mean.array[counter]),
  sigma = matrix(unlist(sigma.array[counter]),2,2)
  ) # generate points from the normal

df.tmp1 <- data.frame(this.cluster.points) # save the generated points in a data frame

df.tmp1$prob <- unlist(lapply(1:num.sample, function(x)dmvnorm(this.cluster.points[x,], mean = unlist(mean.array[counter]), sigma = matrix(unlist(sigma.array[counter]),2,2)))) # compute the pdf of the normal at each sampled point

df.tmp2 <- data.frame(cbind(x.old[which(model$cluster==counter),],0)) # obtain the sampled points belonging to this cluster from stage-one sampling

names(df.tmp2) <- names(df.tmp1)
df.tmp1 <- rbind(df.tmp1, df.tmp2) # combine the two temporary data frames

rbPal <- colorRampPalette(c('red','blue')) # specify color scale.

df.tmp1$Col <- rbPal(nrow(df.tmp1))[as.numeric(cut(df.tmp1$prob, breaks = nrow(df.tmp1)))] # adds a column of color values based on the y values.

plot(x.old,
     col = model$cluster,
     xlim = c(-1.2, 1.2),
     ylim = c(-1.2, 1.2),
     pch = model$cluster,
     main = paste("Sampling from Normal Distribution #", counter))
grid()
points(model$centers, col = 1:size, pch = 2)
text(model$centers[,1], model$centers[,2], labels=as.character(1:size), col="dark orange", pos=c(1), offset=-0.16)

points(df.tmp1$X1[1:num.sample],df.tmp1$X2[1:num.sample],pch = 20,col = alpha(df.tmp1$Col[1:num.sample],0.5))
}

for (j in 1:size){
plot.cluster(j)
# Simulate bivariate normal data, using code from http://blog.revolutionanalytics.com/2016/02/multivariate_data_with_r.html
mu <- unlist(mean.array[j])  # Mean
Sigma <-  matrix(unlist(sigma.array[j]),2,2) # Covariance matrix
bivn <- mvrnorm(5000, mu = mu, Sigma = Sigma )  # from Mass package
head(bivn)
# Calculate kernel density estimate
bivn.kde <- kde2d(bivn[,1], bivn[,2], n = 50)
image(bivn.kde) # from base graphics package
contour(bivn.kde, add = TRUE)
}
```

We generate $(n-m)$ many $x$'s from the GMM. For each $x$, we sample a $y$ from $Y|X=x$. We evaluate $s(y)$. 

```{r}
## ------------------------------------------------------------------------
# # We note that mvrnorm is faster than rmvnorm. Hence we use mvrnorm.
# x.new <- ldply(
#   components,
#   .fun = function(x) {
#     rmvnorm(1,
#             mean = mean.array[[x]],
#             sigma = sigma.array[[x]])
#   }
# )

x.new <- ldply( 
  components,
  .fun = function(x) {
    mvrnorm(1,
            mu = mean.array[[x]],
            Sigma = sigma.array[[x]])
  }
)
y.new <- ldply(
  1:nrow(x.new),
  .fun = function(x)
    v(as.numeric(x.new[x, ]))
 )
s.new <- ldply(
    1:nrow(x.new),
    .fun = function(x) {
      ifelse(y.new[x, ] > zeta, 1, 0)
    }
  )
```

We compute two probabilies of each newly sampled $x$'s in the second stage. One, called "p.old", is from the noisy circle. The other, called "p.new", is from GMM.

```{r}
p.old <- circle.density.df(as.matrix(x.new))
p.new <- cluster.size.percentage[1] * dmvnorm(x.new, mean.array[[1]], sigma.array[[1]])
for (j in 2:size) {
  p.new <- p.new + cluster.size.percentage[j] * dmvnorm(x.new, mean.array[[j]], sigma.array[[j]])
}
```

We visualize the new probabilities.

```{r}
## ----plot new probabilities--------------------------------------------------------------------
hist(
  p.new,
  breaks = seq(min(p.new),
               max(p.new), l = 35),
  col = "purple",
  xlab = "Probabilities from GMM",
  main = "Histogram of p.new, with n-m Probabilities"
)
```

### Finally we can compute the importance sampling estimator

The importance sampling estimator is a weighted mean of the $s(y)$'s in the second stage. It is an unbiased estimator of $0.5$.

```{r}
## ------------------------------------------------------------------------
estimator.new <- mean(s.new[, 1] / p.new * p.old)
estimator.new
```

## The importance sampling estimator as a whole, with visualizations commented out.

```{r}
sample.once.importance <- function(n=2000){
  out <- tryCatch(
    {
      m <- floor (n^(2/3)) # first-stage samples 
      theta <- runif(m, 0, 2 * pi)
      u <- cbind(cos(theta), sin(theta))
      # ## ----plot u--------------------------------------------------------------------
      # plot(u)
      
      epsilon <- mvrnorm(m, mu = c(0, 0), Sigma = 0.01 * diag(2))
      x <- u + epsilon
      # ## ----plot x--------------------------------------------------------------------
      # plot(x)
      
      x.old <- x
      
      v <- function(x) {
        return(rnorm(1, exp(x[1] + x[2]), 1))
      }
      
      ## ------------------------------------------------------------------------
      y <-
        unlist(lapply(1:m, function(x)
          v(x.old[x, ]))) # these are the sampled V's.
      # sum(y == 0)/length(y)   
      # mean(y) # 1.525384
      # median(y) # 1.377392
      
      ## ------------------------------------------------------------------------
      zeta <- 1.35 # this is chosen to be the median of one trial y. Goal is this represents P(Y>zeta)= 0.5.
      s <- ifelse(y > zeta, 1, 0)
      
      
      # ## ----plot0---------------------------------------------------------------
      # color <- ifelse(y > zeta, "blue", "red")
      # plot(x.old,
      #      col = color,
      #      pch = 20,
      #      main = "First stage samples of x, color by values of y's.")
      # grid()
      # legend(
      #   'bottomright',
      #   c("y>10", "y<10"),
      #   pch = 20,
      #   col = c("blue", "red"),
      #   bty = 'y',
      #   cex = .65
      # )
      # 
      ## ------------------------------------------------------------------------
      size <- floor(sqrt(m))  # number of k-means clusters
      model <- kmeans(x.old, size)
      cluster.size.count <-
        unlist(lapply(1:size, function(x) {
          length(which(model$cluster == x))
        }))
      
      # ## ----plot1--------------------------------------------------------------------
      # barplot(
      #   cluster.size.count,
      #   main = "Size of Clusters",
      #   xlab = "Cluster index",
      #   names.arg = 1:size,
      #   col = "purple"
      # )
      
      # ## ----plot2---------------------------------------------------------------
      # plot(x.old,
      #      col = model$cluster,
      #      # xlim = c(-1,1),
      #      # ylim = c(-1,1),
      #      pch = 20)
      # # points(model$centers, col = "black", pch = 2)
      # grid()
      # text(
      #   model$centers[, 1],
      #   model$centers[, 2],
      #   labels = as.character(1:size),
      #   col = "black",
      #   pos = c(1),
      #   offset = -0.16
      # )
      
      ## ------------------------------------------------------------------------
      cluster.prob <-
        unlist(lapply(1:nrow(model$centers), function(x) {
          mean(s[which(model$cluster == x)])
        }))
      
      # ## ----plot3---------------------------------------------------------------
      # barplot(
      #   cluster.prob,
      #   main = "Proportion of Y>10's in the Clusters",
      #   xlab = "Cluster index",
      #   names.arg = 1:size,
      #   col = "purple"
      # )
      
      # ## ----plot4---------------------------------------------------------------
      # pos.prob <- which(cluster.prob > 0)
      # colors.grayscale <-
      #   paste("gray", floor((1 - cluster.prob) * 100), sep = "")
      # plot(
      #   x.old[which(model$cluster == pos.prob[1]), ],
      #   col = colors.grayscale[1],
      #   bg = colors.grayscale[1],
      #   xlim = c(-1.5, 1.5),
      #   ylim = c(-1.5, 1.5),
      #   main = "Clusters and Their Respective Probabilities",
      #   xlab = "x_1",
      #   ylab = "x_2",
      #   pch = 20
      # )
      # grid()
      # for (i in 2:size) {
      #   points(x.old[which(model$cluster == i), ],
      #          col = colors.grayscale[i],
      #          bg = colors.grayscale[i],
      #          pch = 20)
      # }
      # text(
      #   model$centers[, 1],
      #   model$centers[, 2],
      #   labels = paste(
      #     paste("#", as.character(1:size), ":", sep = ""),
      #     paste(floor(cluster.prob * 100), rep("%", size), sep = ""),
      #     sep = " "
      #   ),
      #   col = "red",
      #   pos = c(1),
      #   offset = -0.16
      # )
      # # text(model$centers[,1], model$centers[,2], labels=as.character(1:size), col="red", pos=c(1), offset=-0.16)
      # # text(model$centers[,1],
      # #      model$centers[,2],
      # #      labels=paste(floor(cluster.prob*100), rep("%", size), sep = ""),
      # #      col="red",
      # #      pos=c(4),
      # #      offset=1)
      # 
      
      ## ------------------------------------------------------------------------
      df <- data.frame(model$centers)
      names(df) <- c("centers.old1", "centers.old2")
      df$prob <- cluster.prob
      # centers.new <- matrix(unlist(lapply(1:size,function(x){colMeans(x.old[which(model$cluster == x),])})),,2)
      # df$centers.new1 <- centers.new[,1]
      # df$centers.new2 <- centers.new[,2]
      df$centers.new2 <- df$centers.old2
      df$centers.new1 <- df$centers.old1
      # df
      
      ## ------------------------------------------------------------------------
      sigma.new <- function(i) {
        cov(x.old[which(model$cluster == i), ])
      }
      
      ## ------------------------------------------------------------------------
      cluster.size.percentage <-
        unlist(lapply(1:size, function(x) {
          sum(s[which(model$cluster == x)]) / sum(s)
        }))
      # sum(cluster.size.percentage) # = 1
      
      # ## ----plot5--------------------------------------------------------------------
      # barplot(
      #   cluster.size.percentage,
      #   main = "Probability of Clusters",
      #   xlab = "Cluster index",
      #   names.arg = 1:size,
      #   col = "purple"
      # )
      
      ## ------------------------------------------------------------------------
      components <- 
        sample(1:size,
               prob = cluster.size.percentage,
               size = (n - m),
               replace = TRUE)
      
      # ## ----plot6--------------------------------------------------------------------
      # hist(
      #   components,
      #   col  = "purple",
      #   breaks = c(0:(size + 1)),
      #   prob = TRUE,
      #   xaxt = 'n',
      #   xlab = "Component Index"
      # )
      # # lines(density(components))
      # axis(1,
      #      at = seq(0.5, (size + 1.5), by = 1),
      #      labels = c(1:(size + 2)),
      #      las = 1)
      
      ## ------------------------------------------------------------------------
      sigma.array <- lapply(1:size, function(x) unlist(sigma.new(x)))
      mean.array <- lapply(1:size, function(x) cbind(df$centers.new1, df$centers.new2)[x, ])
      df$sigma <- sigma.array
      # df
      
      # ## -----gmm.unused-------------------------------------------------------------------
      # gmm.pdf <- function(x){ # goal: This function should integrate to 1.
      #   sum(cluster.size.percentage *
      #         unlist(lapply(1:size,
      #                       function(j){dmvnorm(x,
      #                                           mean = mean.array[[j]],
      #                                           sigma = sigma.array[[j]]
      #                       )
      #                       }
      #         )
      #         )
      #   )
      # }
      
      # ## ----plot7.visualize.gmm--------------------------------------------------------------------
      # plot.cluster <- function(counter){ # counter indicate cluster indices
      #
      #   num.sample <- 300 # sample 300 times from each normal.
      #
      #   this.cluster.points <- rmvnorm(
      #     num.sample, mean = unlist(mean.array[counter]),
      #     sigma = matrix(unlist(sigma.array[counter]),2,2)
      #     ) # generate points from the normal
      #
      #   df.tmp1 <- data.frame(this.cluster.points) # save the generated points in a data frame
      #
      #   df.tmp1$prob <- unlist(lapply(1:num.sample, function(x)dmvnorm(this.cluster.points[x,], mean = unlist(mean.array[counter]), sigma = matrix(unlist(sigma.array[counter]),2,2)))) # compute the pdf of the normal at each sampled point
      #
      #   df.tmp2 <- data.frame(cbind(x.old[which(model$cluster==counter),],0)) # obtain the sampled points belonging to this cluster from stage-one sampling
      #
      #   names(df.tmp2) <- names(df.tmp1)
      #   df.tmp1 <- rbind(df.tmp1, df.tmp2) # combine the two temporary data frames
      #
      #   rbPal <- colorRampPalette(c('red','blue')) # specify color scale.
      #
      #   df.tmp1$Col <- rbPal(nrow(df.tmp1))[as.numeric(cut(df.tmp1$prob, breaks = nrow(df.tmp1)))] # adds a column of color values based on the y values.
      #
      #   plot(x.old,
      #        col = model$cluster,
      #        xlim = c(-1.2, 1.2),
      #        ylim = c(-1.2, 1.2),
      #        pch = model$cluster,
      #        main = paste("Sampling from Normal Distribution #", counter))
      #   grid()
      #   points(model$centers, col = 1:size, pch = 2)
      #   text(model$centers[,1], model$centers[,2], labels=as.character(1:size), col="dark orange", pos=c(1), offset=-0.16)
      #
      #   points(df.tmp1$X1[1:num.sample],df.tmp1$X2[1:num.sample],pch = 20,col = alpha(df.tmp1$Col[1:num.sample],0.5))
      # }
      #
      # for (j in 1:size){
      #   plot.cluster(j)
      #   # Simulate bivariate normal data, using code from http://blog.revolutionanalytics.com/2016/02/multivariate_data_with_r.html
      #   mu <- unlist(mean.array[j])  # Mean
      #   Sigma <-  matrix(unlist(sigma.array[j]),2,2) # Covariance matrix
      #   bivn <- mvrnorm(5000, mu = mu, Sigma = Sigma )  # from Mass package
      #   head(bivn)
      #   # Calculate kernel density estimate
      #   bivn.kde <- kde2d(bivn[,1], bivn[,2], n = 50)
      #   image(bivn.kde) # from base graphics package
      #   contour(bivn.kde, add = TRUE)
      # }
      
      
      ## ------------------------------------------------------------------------
      # # We note that mvrnorm is faster than rmvnorm
      # x.new <- ldply(
      #   components,
      #   .fun = function(x) {
      #     rmvnorm(1,
      #             mean = mean.array[[x]],
      #             sigma = sigma.array[[x]])
      #   }
      # )
      x.new <- ldply( 
        components,
        .fun = function(x) {
          mvrnorm(1,
                  mu = mean.array[[x]],
                  Sigma = sigma.array[[x]])
        }
      )
      # y.new <- ldply(
      #     1:nrow(x.new),
      #     .fun = function(x)
      #       v(unlist(x.new[x, ]))
      #   )
      y.new <- ldply(
        1:nrow(x.new),
        .fun = function(x)
          v(as.numeric(x.new[x, ]))
      )
      s.new <- ldply(
          1:nrow(x.new),
          .fun = function(x) {
            ifelse(y.new[x, ] > zeta, 1, 0)
          }
        )
      
      ## ------------------------------------------------------------------------
      # https://stackoverflow.com/questions/38761453/confusion-on-2-dimension-kernel-density-estimation-in-r
      # https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/kde2d.html
      # https://stackoverflow.com/questions/30814545/r-get-joint-probabilities-from-2d-kernel-density-estimate
      # density <- kde2d(x.old[, 1], x.old[, 2], n = 50)
      # points <- data.frame(x = x.new[, 1], y = x.new[, 2])
      # p.old2 <- interp.surface(density, points)
      
      # ## ----plot8.gmm.kde.plot--------------------------------------------------------------------
      # filled.contour(density, color.palette = colorRampPalette(c(
      #   'white', 'blue',
      #   'yellow', 'red',
      #   'darkred'
      # )))

      ## ------------------------------------------------------------------------
      p.old <- circle.density.df(as.matrix(x.new))
      p.new <- cluster.size.percentage[1] * dmvnorm(x.new, mean.array[[1]], sigma.array[[1]])
      for (j in 2:size) {
        p.new <- p.new + cluster.size.percentage[j] * dmvnorm(x.new, mean.array[[j]], sigma.array[[j]])
      }
      
      # ## ----plot9--------------------------------------------------------------------
      # hist(
      #   p.new,
      #   breaks = seq(min(p.new),
      #                max(p.new), l = 35),
      #   col = "purple",
      #   xlab = "Probabilities from GMM",
      #   main = "Histogram of p.new, with n-m Probabilities"
      # )
      
      ## ------------------------------------------------------------------------
      estimator.new <- mean(s.new[, 1] / p.new * p.old)
      estimator.new
      return(estimator.new)
    },
    error = function(cond){
      return(-999)
    }
  )
  return(out)
}
```

## Run our algorithm
```{r, eval=FALSE}
results1000 <- ldply(1:1000, .fun = function(x){sample.once.importance(1000)})
results2000 <- ldply(1:1000, .fun = function(x){sample.once.importance(2000)})
results4000 <- ldply(1:1000, .fun = function(x){sample.once.importance(4000)})
results8000 <- ldply(1:1000, .fun = function(x){sample.once.importance(8000)})
results16000 <- ldply(1:1000, .fun = function(x){sample.once.importance(16000)})

results1000.1<- ldply(1:9000, .fun = function(x){sample.once.importance(1000)})
results2000.1 <- ldply(1:9000, .fun = function(x){sample.once.importance(2000)})
results4000.1 <- ldply(1:9000, .fun = function(x){sample.once.importance(4000)})
results8000.1 <- ldply(1:9000, .fun = function(x){sample.once.importance(8000)})
results16000.1 <- ldply(1:9000, .fun = function(x){sample.once.importance(16000)})

results1000.is <- rbind(results1000, results1000.1)
results2000.is <- rbind(results2000, results2000.1)
results4000.is <- rbind(results4000, results4000.1)
results8000.is <- rbind(results8000, results8000.1)


results1000.naive <- ldply(1:10000, .fun = function(x){sample.once.naive(1000)})
results2000.naive <- ldply(1:10000, .fun = function(x){sample.once.naive(2000)})
results4000.naive <- ldply(1:10000, .fun = function(x){sample.once.naive(4000)})
results6000.naive <- ldply(1:10000, .fun = function(x){sample.once.naive(6000)})
results8000.naive <- ldply(1:10000, .fun = function(x){sample.once.naive(8000)})
```

## Numerical error (TODO)

We note that there are a few estimators that are very large. For example, one estimate for $n=2000$ is $30049.81$. An estimate for $n=4000$ is $33.29$. We know that the estimator should be an unbiased estimator for $0.5$. 

Perhaps there is some numerical error here. I'm guessing error could happen in:

- when I use Gaussian mixture to approaximate the natural distribution of the uniform distribution on a ring plus an iid normal noise. 

```{r}
results1000[which(results1000> 0.9),]
results2000[which(results2000> 0.9),]
results4000[which(results4000> 0.9),]
results8000[which(results8000> 0.9),]
results16000[which(results16000> 0.9),]
```

```{r, echo = FALSE}
# > results8000[which(results8000> 0.9),]
# [1] 3.114918 3.384286
# > results1000[which(results1000> 0.9),]
# [1] 1.293674 1.056567 1.711556 1.058156 1.741305 1.104009
# > results2000[which(results2000> 0.9),]
# [1]     4.859233     8.123297 30049.818837    19.239264
# > results4000[which(results4000> 0.9),]
# [1]    0.959841    4.863888    1.320621   33.298570    0.958705 3816.221748  272.749675   13.148139
# > results8000[which(results8000> 0.9),]
# [1] 3.114918 3.384286
# > results16000[which(results16000> 0.9),]
# numeric(0)
```

## Plot the mean square errors

For demonstration, we have run ten thousand simnulations for $n=1000, 2000, 4000, 8000$ respectively. Let's see what the MSEs are.

Here the darker solid lines are the importance sampler MSE. The lighter dotted lines are the naive sampler MSE.

```{r}
df.tmp <- rbind(results1000,results1000.1)
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
# df.tmp <- data.frame(df.tmp[1:1000,1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
plot(1:nrow(df.tmp), 
     mse.tmp[,1], 
     type = "l", 
     col = "firebrick",
     lty = "solid",
     main = "MSE with Different Numbers of Samples", 
     xlab = "Number of Simulations",
     ylab = "MSE")
df.tmp <- rbind(results2000,results2000.1)
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
lines(1:nrow(df.tmp), 
     mse.tmp[,1], 
     type = "l", 
     col = "aquamarine4",
     lty = "solid")
df.tmp <- rbind(results4000,results4000.1)
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
lines(1:nrow(df.tmp), 
      mse.tmp[,1], 
      type = "l", 
      col = "darkorchid4",
      lty = "solid")
df.tmp <- rbind(results8000,results8000.1)
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
lines(1:nrow(df.tmp), 
      mse.tmp[,1], 
      type = "l", 
      col = "darkgoldenrod3",
      lty = "solid")
grid()
legend('topright', 
       bg="transparent",
       c("n=1000", "n=2000", "n=4000", "n=8000"), 
       lty = c("solid"),
       col = c("firebrick", "aquamarine4", "darkorchid4", "darkgoldenrod3"), 
       bty = 'y', 
       lwd = 2,
       title = "Importance Sampling",
       title.col = "black",
       cex = 0.5)

df.tmp <- results1000.naive
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
lines(1:nrow(df.tmp), 
     mse.tmp[,1], 
     type = "l", 
     col = "firebrick1",
     lty = "dotted",
     main = "MSE with n = 2000", 
     xlab = "Number of Simulations",
     ylab = "MSE")
df.tmp <- results2000.naive
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
lines(1:nrow(df.tmp), 
      mse.tmp[,1], 
      type = "l", 
      col = "mediumaquamarine",
      lty = "dotted")
df.tmp <- results4000.naive
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
lines(1:nrow(df.tmp), 
      mse.tmp[,1], 
      type = "l", 
      col = "darkorchid1",
      lty = "dotted")
df.tmp <- results8000.naive
df.tmp <- data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1])
mse.tmp <- ldply(1:nrow(df.tmp), .fun = function(x){mean((df.tmp[1:x,1]-0.5)^2)})
lines(1:nrow(df.tmp), 
      mse.tmp[,1], 
      type = "l", 
      col = "darkgoldenrod1",
      lty = "dotted")
legend('topleft', 
       bg="transparent",
       c("n=1000", "n=2000", "n=4000", "n=8000"), 
       lty = c("dotted"),
       col = c("firebrick1", "mediumaquamarine", "darkorchid1", "darkgoldenrod1"), 
       bty = 'y', 
       lwd = 2,
       title = "Naive Sampling",
       title.col = "black",
       cex = 0.5)
```

We see that the MSE for each fixed $n$ roughly approaches a horizontal asymptote. As $n$ increases, the limiting MSE decreases.

## Scale MSE by n

We scale the MSE by a multiplicative factor of $n$. Then the MSE stabilizes at around $2$.

```{r}
df.tmp <- results1000.is
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] # 0.002162626
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] * 1000 # 2.162626

df.tmp <- results2000.is
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] #  0.001021574
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] * 2000 # 2.043148

df.tmp <- results4000.is
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] # 0.0005297752
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] * 4000 # 2.119101

df.tmp <- results8000.is
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] # 0.0002019861
var(data.frame(df.tmp[which(df.tmp>0 & df.tmp<0.9),1]))[1] * 8000 # 1.615889
```

## Future work

We end the notebook here. In the next note, we will focus on visualizing the spread of the estimators given in this note. We will also add more dummy dimensions to the natural distribution and see how that affects the two estimators.